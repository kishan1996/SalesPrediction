{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Phases:\n",
    "* Libraries and Data Loading\n",
    "* Exploratory Analysis and Data Cleaning\n",
    "* Machine Learning\n",
    "* Christmas Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries and Data Loading \n",
    "\n",
    "We will be loading some libraries to make processing easier and some to view the data\n",
    "libraries used :\n",
    "* Pandas\n",
    "* Numpy\n",
    "* MatPlotLib\n",
    "* Seaborn\n",
    "* SciPy\n",
    "* SkLearn\n",
    "* PandaSQL\n",
    "* Warning (ignore any warning we might come across)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # ignoring annoying warnings\n",
    "\n",
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "import pickle # for saving the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandasql in c:\\users\\kishan menaria\\anaconda3\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\kishan menaria\\anaconda3\\lib\\site-packages (from pandasql) (1.3.4)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\kishan menaria\\anaconda3\\lib\\site-packages (from pandasql) (1.4.22)\n",
      "Requirement already satisfied: numpy in c:\\users\\kishan menaria\\anaconda3\\lib\\site-packages (from pandasql) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\kishan menaria\\anaconda3\\lib\\site-packages (from pandas->pandasql) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\kishan menaria\\anaconda3\\lib\\site-packages (from pandas->pandasql) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kishan menaria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->pandasql) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\kishan menaria\\anaconda3\\lib\\site-packages (from sqlalchemy->pandasql) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kaggle/inputs/dataset/features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KISHAN~1\\AppData\\Local\\Temp/ipykernel_9992/3578907098.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kaggle/inputs/dataset/features.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kaggle/inputs/dataset/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kaggle/inputs/dataset/stores.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kaggle/inputs/dataset/test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msample_submission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kaggle/inputs/dataset/sampleSubmission.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kaggle/inputs/dataset/features.csv'"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('kaggle/inputs/dataset/features.csv')\n",
    "train = pd.read_csv('kaggle/inputs/dataset/train.csv')\n",
    "stores = pd.read_csv('kaggle/inputs/dataset/stores.csv')\n",
    "test = pd.read_csv('kaggle/inputs/dataset/test.csv')\n",
    "sample_submission = pd.read_csv('kaggle/inputs/dataset/sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Analysis and Data Cleaning\n",
    "* Merging \"Features\" and \"Stores\"\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_sto = features.merge(stores, how='inner', on='Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_sto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(feat_sto.dtypes, columns=['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'Type_Train': train.dtypes, 'Type_Test': test.dtypes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_sto.Date = pd.to_datetime(feat_sto.Date)\n",
    "train.Date = pd.to_datetime(train.Date)\n",
    "test.Date = pd.to_datetime(test.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_sto['Week'] = feat_sto.Date.dt.week \n",
    "feat_sto['Year'] = feat_sto.Date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_detail = train.merge(feat_sto, \n",
    "                           how='inner',\n",
    "                           on=['Store','Date','IsHoliday']).sort_values(by=['Store','Dept','Date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_detail.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_detail = test.merge(feat_sto, \n",
    "                           how='inner',\n",
    "                           on=['Store','Date','IsHoliday']).sort_values(by=['Store',\n",
    "                                                                            'Dept',\n",
    "                                                                            'Date']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del features, train, stores, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "null_columns = (train_detail.isnull().sum(axis = 0)/len(train_detail)).sort_values(ascending=False).index\n",
    "null_data = pd.concat([\n",
    "    train_detail.isnull().sum(axis = 0),\n",
    "    (train_detail.isnull().sum(axis = 0)/len(train_detail)).sort_values(ascending=False),\n",
    "    train_detail.loc[:, train_detail.columns.isin(list(null_columns))].dtypes], axis=1)\n",
    "null_data = null_data.rename(columns={0: '# null', \n",
    "                                      1: '% null', \n",
    "                                      2: 'type'}).sort_values(ascending=False, by = '% null')\n",
    "null_data = null_data[null_data[\"# null\"]!=0]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holidays Analysis\n",
    "\n",
    "If, for a certain Week, there are more pre-holiday days in one Year than another, then it is very possible that the Year with more pre-holiday days will have greater Sales for the same Week. So, the model will not take this consideration and we might need to adjust the predicted values at the end.\n",
    "\n",
    "Another thing to take into account is that Holiday Weeks but with few or no pre-holiday days might have lower Sales than the Week before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pysqldf(\"\"\"\n",
    "SELECT\n",
    "    T.*,\n",
    "    case\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 1 then 'Super Bowl'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 2 then 'Labor Day'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 3 then 'Thanksgiving'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 then 'Christmas'\n",
    "    end as Holyday,\n",
    "    case\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 1 then 'Sunday'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 2 then 'Monday'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 3 then 'Thursday'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2010 then 'Saturday'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2011 then 'Sunday'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2012 then 'Tuesday'\n",
    "    end as Day\n",
    "    from(\n",
    "        SELECT DISTINCT\n",
    "            Year,\n",
    "            Week,\n",
    "            case \n",
    "                when Date <= '2012-11-01' then 'Train Data' else 'Test Data' \n",
    "            end as Data_type\n",
    "        FROM feat_sto\n",
    "        WHERE IsHoliday = True) as T\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Observation 1**\n",
    "\n",
    "* All Holidays fall on the same week\n",
    "* Christmas has 0 pre-holiday days in 2010, 1 in 2011 and 3 in 2012. The model will not consider more Sales in 2012 for Test Data, so we are going to adjust it at the end, with a formula and an explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Weekly Sales - Per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_detail' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KISHAN~1\\AppData\\Local\\Temp/ipykernel_9992/3663161975.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mweekly_sales_2010\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_detail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2010\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Weekly_Sales'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Week'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mweekly_sales_2011\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_detail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2011\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Weekly_Sales'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Week'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mweekly_sales_2012\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_detail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2012\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Weekly_Sales'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Week'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweekly_sales_2010\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweekly_sales_2010\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_detail' is not defined"
     ]
    }
   ],
   "source": [
    "weekly_sales_2010 = train_detail[train_detail.Year==2010]['Weekly_Sales'].groupby(train_detail['Week']).mean()\n",
    "weekly_sales_2011 = train_detail[train_detail.Year==2011]['Weekly_Sales'].groupby(train_detail['Week']).mean()\n",
    "weekly_sales_2012 = train_detail[train_detail.Year==2012]['Weekly_Sales'].groupby(train_detail['Week']).mean()\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.lineplot(weekly_sales_2010.index, weekly_sales_2010.values)\n",
    "sns.lineplot(weekly_sales_2011.index, weekly_sales_2011.values)\n",
    "sns.lineplot(weekly_sales_2012.index, weekly_sales_2012.values)\n",
    "plt.grid()\n",
    "plt.xticks(np.arange(1, 53, step=1))\n",
    "plt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\n",
    "plt.title('Average Weekly Sales - Per Year', fontsize=18)\n",
    "plt.ylabel('Sales', fontsize=16)\n",
    "plt.xlabel('Week', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation 2\n",
    "As we can see, there is one important Holiday not included in 'IsHoliday'. It's the Easter Day. It is always in a Sunday, but can fall on different weeks.\n",
    "\n",
    "In 2010 is in Week 13\n",
    "In 2011, Week 16\n",
    "Week 14 in 2012\n",
    "and, finally, Week 13 in 2013 for Test set\n",
    "So, we can change to 'True' these Weeks in each Year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Adding Easter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_detail' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KISHAN~1\\AppData\\Local\\Temp/ipykernel_9992/3779305751.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2010\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWeek\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IsHoliday'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2011\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWeek\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IsHoliday'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2012\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWeek\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IsHoliday'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYear\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2013\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_detail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWeek\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IsHoliday'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_detail' is not defined"
     ]
    }
   ],
   "source": [
    "train_detail.loc[(train_detail.Year==2010) & (train_detail.Week==13), 'IsHoliday'] = True\n",
    "train_detail.loc[(train_detail.Year==2011) & (train_detail.Week==16), 'IsHoliday'] = True\n",
    "train_detail.loc[(train_detail.Year==2012) & (train_detail.Week==14), 'IsHoliday'] = True\n",
    "test_detail.loc[(test_detail.Year==2013) & (test_detail.Week==13), 'IsHoliday'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Sales - Mean and Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weekly_sales_mean = train_detail['Weekly_Sales'].groupby(train_detail['Date']).mean()\n",
    "weekly_sales_median = train_detail['Weekly_Sales'].groupby(train_detail['Date']).median()\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.lineplot(weekly_sales_mean.index, weekly_sales_mean.values)\n",
    "sns.lineplot(weekly_sales_median.index, weekly_sales_median.values)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend(['Mean', 'Median'], loc='best', fontsize=16)\n",
    "plt.title('Weekly Sales - Mean and Median', fontsize=18)\n",
    "plt.ylabel('Sales', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Sales per Store and Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Store']).mean()\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(weekly_sales.index, weekly_sales.values, palette='dark')\n",
    "plt.grid()\n",
    "plt.title('Average Sales - per Store', fontsize=18)\n",
    "plt.ylabel('Sales', fontsize=16)\n",
    "plt.xlabel('Store', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Sales - Department Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Dept']).mean()\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(weekly_sales.index, weekly_sales.values, palette='dark')\n",
    "plt.grid()\n",
    "plt.title('Average Sales - per Dept', fontsize=18)\n",
    "plt.ylabel('Sales', fontsize=16)\n",
    "plt.xlabel('Dept', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obeservation 3\n",
    "There are Sales difference between the Store and Departments. Also some Depts are not in the list, like number '15', for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix - Pearson Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Metrics:\n",
    "\n",
    "* 0: no correlation at all\n",
    "* 0-0.3: weak correlation\n",
    "* 0.3-0.7: moderate correlaton\n",
    "* 0.7-1: strong correlation\n",
    "\n",
    "**Positive Correlation indicates that when one variable increase, the other also does. Negative is the opposite.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "corr = train_detail.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "plt.title('Correlation Matrix', fontsize=18)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 4\n",
    "'**MarkDown**' 1 to 5 are not strong correlated to '**Weekly_Sales**' and they have a lot of null values, then we can drop them.\n",
    "\n",
    "Also, '**Fuel_Price**' is strong correlated to '**Year**'. One of them must be dropped else they would carry similar information to the model. 'Year' will not be dropped, because it differentiate same Weeks for 'Store'+'Dept'.\n",
    "\n",
    "Other variables that have weak correlation with '**Weekly_Sales**' can be analyzed to see if they are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Droping Markdown values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_detail = train_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])\n",
    "test_detail = test_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Variables\n",
    "1. BoxPlot and StripPlot for Discrete Variables\n",
    "2. BoxCox for Continous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def boxplot(feature):\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "    gs = GridSpec(1,2)\n",
    "    sns.boxplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,0]))\n",
    "    plt.ylabel('Sales', fontsize=16)\n",
    "    plt.xlabel(feature, fontsize=16)\n",
    "    sns.stripplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,1]))\n",
    "    plt.ylabel('Sales', fontsize=16)\n",
    "    plt.xlabel(feature, fontsize=16)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def boxcox(feature):\n",
    "    \n",
    "    fig = plt.figure(figsize=(18,15))\n",
    "    gs = GridSpec(2,2)\n",
    "    \n",
    "    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n",
    "                        x=boxcox1p(train_detail[feature], 0.15), ax=fig.add_subplot(gs[0,1]), palette = 'blue')\n",
    "\n",
    "    plt.title('BoxCox 0.15\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.15)),2)) +\n",
    "              ', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.15), nan_policy='omit'),2)))\n",
    "    \n",
    "    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n",
    "                        x=boxcox1p(train_detail[feature], 0.25), ax=fig.add_subplot(gs[1,0]), palette = 'blue')\n",
    "\n",
    "    plt.title('BoxCox 0.25\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.25)),2)) +\n",
    "              ', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.25), nan_policy='omit'),2)))\n",
    "    \n",
    "    j = sns.distplot(train_detail[feature], ax=fig.add_subplot(gs[1,1]), color = 'green')\n",
    "\n",
    "    plt.title('Distribution\\n')\n",
    "    \n",
    "    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n",
    "                        x=train_detail[feature], ax=fig.add_subplot(gs[0,0]), color = 'red')\n",
    "\n",
    "    plt.title('Linear\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(train_detail[feature]),2)) + ', Skew: ' + \n",
    "               str(np.round(stats.skew(train_detail[feature], nan_policy='omit'),2)))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Sales vs IsHoliday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boxplot('IsHoliday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boxplot('Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know what 'Type' is, but we can assume that A > B > C in terms of Sales Median. So, let's treat it as an ordinal variable and replace its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_detail.Type = train_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\n",
    "test_detail.Type = test_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Sales vs Temperature (Continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boxcox('Temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations doesn't seem to change at any skewness toh we will drop this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droping Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_detail = train_detail.drop(columns=['Temperature'])\n",
    "test_detail = test_detail.drop(columns=['Temperature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Sales vs Unemployement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boxcox('Unemployment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for **Unemployment** rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detail = train_detail.drop(columns=['Unemployment'])\n",
    "test_detail = test_detail.drop(columns=['Unemployment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Sales vs CPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxcox('CPI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for **CPI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detail = train_detail.drop(columns=['CPI'])\n",
    "test_detail = test_detail.drop(columns=['CPI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Sales vs Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxcox('Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue with this variable, since it has moderate correlation with **WeeklySales**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Mean Average Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WMAE(dataset, real, predicted):\n",
    "    weights = dataset.IsHoliday.apply(lambda x: 5 if x else 1)\n",
    "    return np.round(np.sum(weights*abs(real-predicted))/(np.sum(weights)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms (Without thinking about efficency)\n",
    "* After trying different algorithms we concluded that we would get the best results by choosing Random Forest and optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors(WMAE) for Different Models:\n",
    "   * knn - 3789.4\n",
    "   * Extra Tree - 4024.48\n",
    "   * Random Forest - 3672\n",
    "   * Linear Regression - 3332.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn():\n",
    "    knn = KNeighborsRegressor(n_neighbors=10)\n",
    "    return knn\n",
    "\n",
    "def extraTreesRegressor():\n",
    "    clf = ExtraTreesRegressor(n_estimators=60, max_features=3, verbose=1, n_jobs=1)\n",
    "    return clf\n",
    "    \n",
    "def randomForestRegressor():\n",
    "    clf = RandomForestRegressor(n_estimators = 100, max_features = 'log2', verbose = 1, bootstrap = True)\n",
    "    return clf\n",
    "\n",
    "def linear_reg():\n",
    "    regr = LinearRegression()\n",
    "    return regr\n",
    "\n",
    "def predict_(m, test_x):\n",
    "    return pd.Series(m.predict(test_x))\n",
    "\n",
    "def model_():\n",
    "#     return knn()\n",
    "#     return extraTreesRegressor()\n",
    "    return randomForestRegressor()   \n",
    "#     return linear_reg()\n",
    "\n",
    "def train_(train_x, train_y):\n",
    "    m = model_()\n",
    "    m.fit(train_x, train_y)\n",
    "    return m\n",
    "\n",
    "def train_and_predict(train_x, train_y, test_x):\n",
    "    m = train_(train_x, train_y)\n",
    "    return predict_(m, test_x), m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "splited = []\n",
    "# dataset2 = dataset.copy()\n",
    "for name, group in train_detail.groupby([\"Store\", \"Dept\"]):\n",
    "    group = group.reset_index(drop=True)\n",
    "    trains_x = []\n",
    "    trains_y = []\n",
    "    tests_x = []\n",
    "    tests_y = []\n",
    "    if group.shape[0] <= 5:\n",
    "        f = np.array(range(5))\n",
    "        np.random.shuffle(f)\n",
    "        group['fold'] = f[:group.shape[0]]\n",
    "        continue\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(group):\n",
    "        group.loc[test_index, 'fold'] = fold\n",
    "        fold += 1\n",
    "    splited.append(group)\n",
    "\n",
    "splited = pd.concat(splited).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "error_cv = 0\n",
    "best_error = np.iinfo(np.int32).max\n",
    "for fold in range(5):\n",
    "    train_detail2 = splited.loc[splited['fold'] != fold]\n",
    "    test_detail2 = splited.loc[splited['fold'] == fold]\n",
    "    train_y = train_detail2['Weekly_Sales']\n",
    "    train_x = train_detail2[['Store','Dept','IsHoliday','Size','Week','Type','Year']]\n",
    "    test_y = test_detail2['Weekly_Sales']\n",
    "    test_x = test_detail2[['Store', 'Dept', 'IsHoliday', 'Size', 'Week', 'Type', 'Year']]\n",
    "    print(train_detail2.shape, test_detail2.shape)\n",
    "    predicted, model = train_and_predict(train_x, train_y, test_x)\n",
    "#     weights = test_x['isHoliday'].replace(True, 5).replace(False, 1)\n",
    "#     error = calculate_error(test_y, predicted, weights)\n",
    "    error = WMAE(test_x, test_y, predicted)\n",
    "    print(error)\n",
    "    error_cv += error\n",
    "    print(fold, error)\n",
    "    if error < best_error:\n",
    "        print('Find best model')\n",
    "        best_error = error\n",
    "        best_model = model\n",
    "error_cv /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_cv # Average Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_error #least error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (Split into 3 parts for optimizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(n_estimators, max_depth):\n",
    "    result = []\n",
    "    for estimator in n_estimators:\n",
    "        for depth in max_depth:\n",
    "            wmaes_cv = []\n",
    "            for i in range(1,5):\n",
    "                print('k:', i, ', n_estimators:', estimator, ', max_depth:', depth)\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n",
    "                RF = RandomForestRegressor(n_estimators=estimator, max_depth=depth)\n",
    "                RF.fit(x_train, y_train)\n",
    "                predicted = RF.predict(x_test)\n",
    "                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n",
    "            print('WMAE:', np.mean(wmaes_cv))\n",
    "            result.append({'Max_Depth': depth, 'Estimators': estimator, 'WMAE': np.mean(wmaes_cv)})\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_II(n_estimators, max_depth, max_features):\n",
    "    result = []\n",
    "    for feature in max_features:\n",
    "        wmaes_cv = []\n",
    "        for i in range(1,5):\n",
    "            print('k:', i, ', max_features:', feature)\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n",
    "            RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=feature)\n",
    "            RF.fit(x_train, y_train)\n",
    "            predicted = RF.predict(x_test)\n",
    "            wmaes_cv.append(WMAE(x_test, y_test, predicted))\n",
    "        print('WMAE:', np.mean(wmaes_cv))\n",
    "        result.append({'Max_Feature': feature, 'WMAE': np.mean(wmaes_cv)})\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_III(n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf):\n",
    "    result = []\n",
    "    for split in min_samples_split:\n",
    "        for leaf in min_samples_leaf:\n",
    "            wmaes_cv = []\n",
    "            for i in range(1,5):\n",
    "                print('k:', i, ', min_samples_split:', split, ', min_samples_leaf:', leaf)\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n",
    "                RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, \n",
    "                                           min_samples_leaf=leaf, min_samples_split=split)\n",
    "                RF.fit(x_train, y_train)\n",
    "                predicted = RF.predict(x_test)\n",
    "                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n",
    "            print('WMAE:', np.mean(wmaes_cv))\n",
    "            result.append({'Min_Samples_Leaf': leaf, 'Min_Samples_Split': split, 'WMAE': np.mean(wmaes_cv)})\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_detail[['Store','Dept','IsHoliday','Size','Week','Type','Year']]\n",
    "Y_train = train_detail['Weekly_Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning n_estimators and max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning 'n_estimators' and 'max_depth'.\n",
    "\n",
    "Here, it is possible to test a lot of values. These are the final ones, after a bit of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [56, 58, 60]\n",
    "max_depth = [25, 27, 30]\n",
    "\n",
    "random_forest(n_estimators, max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result based on WAME \\\n",
    "Max_Depth - 30 \\\n",
    "Estimators - 56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = [2, 3, 4, 5, 6, 7]\n",
    "\n",
    "random_forest_II(n_estimators=56, max_depth=30, max_features=max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max_Feature - 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning min_samples_spilt and min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split = [2, 3, 4]\n",
    "min_samples_leaf = [1, 2, 3]\n",
    "\n",
    "random_forest_III(n_estimators=56, max_depth=30, max_features=7, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min_Samples_Leaf - 1 \\\n",
    "Min_Samples_Split - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestRegressor(n_estimators=56, max_depth=30, max_features=7, min_samples_split=3, min_samples_leaf=1)\n",
    "RF.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_file = open(\"SavedModel.sav\", 'wb')\n",
    "pickle.dump(RF, model_save_file)\n",
    "model_save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_saved_model_file = open('SavedModel.sav', 'rb')\n",
    "loaded_model = pickle.load(load_saved_model_file)\n",
    "load_saved_model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_detail[['Store', 'Dept', 'IsHoliday', 'Size', 'Week', 'Type', 'Year']]\n",
    "loaded_predict = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_detail[['Store', 'Dept', 'IsHoliday', 'Size', 'Week', 'Type', 'Year']]\n",
    "predict = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.all(loaded_predict==predict)) # to show that saved model is loading perfectly by comparing the predictions\n",
    "# True if all elements are equal, False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['Weekly_Sales'] = loaded_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error(WMAE) on Walmart's testing data (hidden)\n",
    "# 2776.28705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "# function that takes in a dataframe and creates a text link to  \n",
    "# download it (will only work for files < 2MB or so)\n",
    "def create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n",
    "    csv = df.to_csv(index=False)\n",
    "    b64 = base64.b64encode(csv.encode())\n",
    "    payload = b64.decode()\n",
    "    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "    html = html.format(payload=payload,title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "# create a link to download the dataframe\n",
    "create_download_link(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
